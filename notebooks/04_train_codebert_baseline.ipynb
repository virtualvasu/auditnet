{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb61d65",
   "metadata": {},
   "source": [
    "# 04 - Train CodeBERT Baseline Model\n",
    "\n",
    "This notebook implements the training pipeline for fine-tuning CodeBERT on Solidity smart contract vulnerability detection.\n",
    "\n",
    "**Objectives:**\n",
    "- Load pre-trained CodeBERT model and add classification head\n",
    "- Implement training loop with AdamW optimizer and learning rate scheduler\n",
    "- Monitor validation F1 score and save best checkpoint\n",
    "- Save final trained model for inference\n",
    "- Log training metrics and hyperparameters\n",
    "\n",
    "**Output:** Trained model checkpoint ready for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cf494",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9241f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Environment Setup\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch and transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Metrics and visualization\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style and disable warnings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üöÄ Training Environment Setup\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf76b9",
   "metadata": {},
   "source": [
    "## Load Configuration and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0024ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset configuration...\n",
      "Model: microsoft/codebert-base\n",
      "Max sequence length: 512\n",
      "Batch size: 16\n",
      "\n",
      "üìä Loading datasets...\n",
      "Train: 10,032 samples\n",
      "Validation: 3,344 samples\n",
      "Test: 3,344 samples\n",
      "\n",
      "üè∑Ô∏è Label classes: ['Overflow-Underflow' 'Re-entrancy' 'SAFE' 'TOD' 'Timestamp-Dependency'\n",
      " 'Unchecked-Send' 'Unhandled-Exceptions' 'tx.origin']\n",
      "Number of classes: 8\n",
      "\n",
      "‚úÖ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset configuration\n",
    "print(\"üìÇ Loading dataset configuration...\")\n",
    "\n",
    "with open('../data/processed/dataset_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "MODEL_NAME = config['model_name']\n",
    "MAX_LENGTH = config['max_length']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nüìä Loading datasets...\")\n",
    "train_df = pd.read_csv('../data/processed/train_functions.csv')\n",
    "val_df = pd.read_csv('../data/processed/validation_functions.csv')\n",
    "test_df = pd.read_csv('../data/processed/test_functions.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df):,} samples\")\n",
    "print(f\"Validation: {len(val_df):,} samples\")\n",
    "print(f\"Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Load label encoder for multiclass\n",
    "with open('../data/processed/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Label classes: {label_encoder.classes_}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../results/checkpoints', exist_ok=True)\n",
    "os.makedirs('../results/metrics', exist_ok=True)\n",
    "os.makedirs('../logs', exist_ok=True)\n",
    "\n",
    "print(\"\\n‚úÖ Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca22be2",
   "metadata": {},
   "source": [
    "## Define Dataset and Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca21f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset and model classes defined!\n"
     ]
    }
   ],
   "source": [
    "class VulnDataset(Dataset):\n",
    "    \"\"\"Dataset for vulnerability detection with both binary and multiclass support\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=512, task='binary'):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.task = task\n",
    "        \n",
    "        if task == 'binary':\n",
    "            self.labels = self.df['is_vulnerable'].astype(int).values\n",
    "        else:  # multiclass\n",
    "            self.labels = label_encoder.transform(self.df['vulnerability_category']).astype(int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get function source code - column name is 'code' not 'source_code'\n",
    "        text = str(self.df.iloc[idx]['code'])\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class CodeBERTClassifier(nn.Module):\n",
    "    \"\"\"CodeBERT with classification head\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout=0.1, freeze_base=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load CodeBERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze base model if requested\n",
    "        if freeze_base:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, return_attention=False):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=return_attention,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        \n",
    "        # Apply dropout and classify\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        result = {'logits': logits}\n",
    "        \n",
    "        # Add attention weights if requested\n",
    "        if return_attention:\n",
    "            result['attentions'] = outputs.attentions\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        if labels is not None:\n",
    "            if self.num_classes == 1:  # Binary classification\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fn(logits.view(-1), labels.float())\n",
    "            else:  # Multiclass\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                loss = loss_fn(logits, labels)\n",
    "            result['loss'] = loss\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ Dataset and model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cb930",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0708ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Training Configuration:\n",
      "  task: binary\n",
      "  dropout: 0.1\n",
      "  freeze_base: False\n",
      "  epochs: 5\n",
      "  learning_rate: 2e-05\n",
      "  weight_decay: 0.01\n",
      "  warmup_steps: 500\n",
      "  max_grad_norm: 1.0\n",
      "  eval_steps: 200\n",
      "  save_steps: 500\n",
      "  logging_steps: 50\n",
      "  early_stopping_patience: 3\n",
      "  scheduler_type: linear\n",
      "  num_workers: 0\n",
      "  pin_memory: False\n",
      "  use_amp: False\n",
      "  use_subset: False\n",
      "\n",
      "üè∑Ô∏è Task: binary (1 class)\n",
      "üèÉ Run name: codebert_binary_20251110_234127\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    # Model settings\n",
    "    'task': 'binary',  # Change to 'multiclass' for multiclass classification\n",
    "    'dropout': 0.1,\n",
    "    'freeze_base': False,  # Set True to freeze CodeBERT base layers\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    'eval_steps': 200,  # Evaluate every N steps\n",
    "    'save_steps': 500,  # Save checkpoint every N steps\n",
    "    'logging_steps': 50,  # Log metrics every N steps\n",
    "    'early_stopping_patience': 3,  # Stop if no improvement for N evaluations\n",
    "    \n",
    "    # Scheduler\n",
    "    'scheduler_type': 'linear',  # 'linear', 'cosine', or 'constant'\n",
    "    \n",
    "    # Data loading\n",
    "    'num_workers': 4 if torch.cuda.is_available() else 0,\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "    \n",
    "    # Mixed precision training\n",
    "    'use_amp': torch.cuda.is_available(),  # Automatic Mixed Precision\n",
    "    \n",
    "    # For quick testing - set to True to use smaller subset\n",
    "    'use_subset': False,  # Set to True for quick testing\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Determine number of classes\n",
    "if TRAINING_CONFIG['task'] == 'binary':\n",
    "    NUM_CLASSES = 1  # Binary classification uses BCEWithLogitsLoss\n",
    "else:\n",
    "    NUM_CLASSES = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Task: {TRAINING_CONFIG['task']} ({NUM_CLASSES} {'class' if NUM_CLASSES == 1 else 'classes'})\")\n",
    "\n",
    "# Create timestamp for this training run\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_NAME = f\"codebert_{TRAINING_CONFIG['task']}_{TIMESTAMP}\"\n",
    "\n",
    "print(f\"üèÉ Run name: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d5db7",
   "metadata": {},
   "source": [
    "## Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7fc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Loading tokenizer...\n",
      "üìä Creating full datasets...\n",
      "Full sizes - Train: 10032, Val: 3344\n",
      "Batch size: 16\n",
      "Train batches: 627\n",
      "Validation batches: 209\n",
      "\n",
      "üß™ Testing data loader...\n",
      "Sample batch shapes:\n",
      "  input_ids: torch.Size([16, 512])\n",
      "  attention_mask: torch.Size([16, 512])\n",
      "  labels: torch.Size([16])\n",
      "‚úÖ Data loaders created successfully!\n",
      "üìä Creating full datasets...\n",
      "Full sizes - Train: 10032, Val: 3344\n",
      "Batch size: 16\n",
      "Train batches: 627\n",
      "Validation batches: 209\n",
      "\n",
      "üß™ Testing data loader...\n",
      "Sample batch shapes:\n",
      "  input_ids: torch.Size([16, 512])\n",
      "  attention_mask: torch.Size([16, 512])\n",
      "  labels: torch.Size([16])\n",
      "‚úÖ Data loaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Use subset for quick testing if enabled\n",
    "if TRAINING_CONFIG['use_subset']:\n",
    "    print(\"‚ö° Using subset for quick testing...\")\n",
    "    # Load pre-saved subset\n",
    "    subset_data = torch.load('../data/processed/subset_datasets.pt')\n",
    "    train_loader = subset_data['train_subset_loader']\n",
    "    val_loader = subset_data['val_subset_loader']\n",
    "    \n",
    "    print(f\"Subset sizes - Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}\")\n",
    "else:\n",
    "    print(\"üìä Creating full datasets...\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VulnDataset(\n",
    "        train_df, \n",
    "        tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        task=TRAINING_CONFIG['task']\n",
    "    )\n",
    "    \n",
    "    val_dataset = VulnDataset(\n",
    "        val_df, \n",
    "        tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        task=TRAINING_CONFIG['task']\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=TRAINING_CONFIG['num_workers'],\n",
    "        pin_memory=TRAINING_CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=TRAINING_CONFIG['num_workers'],\n",
    "        pin_memory=TRAINING_CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    print(f\"Full sizes - Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}\")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nüß™ Testing data loader...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shapes:\")\n",
    "print(f\"  input_ids: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {sample_batch['labels'].shape}\")\n",
    "\n",
    "print(\"‚úÖ Data loaders created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe427b32",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0158ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing CodeBERT model...\n",
      "Model loaded on cpu\n",
      "Total parameters: 124,646,401\n",
      "Trainable parameters: 124,646,401\n",
      "\n",
      "üìÖ Training schedule:\n",
      "  Total steps: 3,135\n",
      "  Warmup steps: 500\n",
      "Scheduler: linear\n",
      "Mixed precision: False\n",
      "\n",
      "‚úÖ Model and training components initialized!\n",
      "Model loaded on cpu\n",
      "Total parameters: 124,646,401\n",
      "Trainable parameters: 124,646,401\n",
      "\n",
      "üìÖ Training schedule:\n",
      "  Total steps: 3,135\n",
      "  Warmup steps: 500\n",
      "Scheduler: linear\n",
      "Mixed precision: False\n",
      "\n",
      "‚úÖ Model and training components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(f\"ü§ñ Initializing CodeBERT model...\")\n",
    "model = CodeBERTClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=TRAINING_CONFIG['dropout'],\n",
    "    freeze_base=TRAINING_CONFIG['freeze_base']\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=TRAINING_CONFIG['learning_rate'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = len(train_loader) * TRAINING_CONFIG['epochs']\n",
    "warmup_steps = TRAINING_CONFIG['warmup_steps']\n",
    "\n",
    "print(f\"\\nüìÖ Training schedule:\")\n",
    "print(f\"  Total steps: {total_steps:,}\")\n",
    "print(f\"  Warmup steps: {warmup_steps:,}\")\n",
    "\n",
    "# Initialize scheduler\n",
    "if TRAINING_CONFIG['scheduler_type'] == 'linear':\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "elif TRAINING_CONFIG['scheduler_type'] == 'cosine':\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "else:  # constant\n",
    "    scheduler = None\n",
    "\n",
    "print(f\"Scheduler: {TRAINING_CONFIG['scheduler_type']}\")\n",
    "\n",
    "# Initialize mixed precision scaler if using AMP\n",
    "scaler = torch.cuda.amp.GradScaler() if TRAINING_CONFIG['use_amp'] else None\n",
    "print(f\"Mixed precision: {TRAINING_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model and training components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f66960",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af3e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(predictions, labels, task='binary'):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    if task == 'binary':\n",
    "        # Convert probabilities to predictions\n",
    "        preds = (predictions > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(labels, predictions)\n",
    "            avg_precision = average_precision_score(labels, predictions)\n",
    "        except ValueError:\n",
    "            auc = 0.0\n",
    "            avg_precision = 0.0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'avg_precision': avg_precision\n",
    "        }\n",
    "    \n",
    "    else:  # multiclass\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device, task='binary'):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert logits to predictions\n",
    "            if task == 'binary':\n",
    "                predictions = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "            else:\n",
    "                predictions = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    metrics = calculate_metrics(all_predictions, all_labels, task)\n",
    "    metrics['loss'] = avg_loss\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, step, metrics, filepath):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'metrics': metrics,\n",
    "        'config': TRAINING_CONFIG,\n",
    "        'model_config': {\n",
    "            'model_name': MODEL_NAME,\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'max_length': MAX_LENGTH\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"üíæ Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d49f4e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training: codebert_binary_20251110_234127\n",
      "Training for 5 epochs...\n",
      "============================================================\n",
      "\n",
      "üìÖ Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebd05810bbc44f18b01d8563d4de151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training tracking\n",
    "training_stats = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'val_metrics': [],\n",
    "    'learning_rates': [],\n",
    "    'epochs': [],\n",
    "    'steps': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_f1 = 0.0\n",
    "best_model_path = f'../results/checkpoints/best_model_{RUN_NAME}.pt'\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Start training\n",
    "print(f\"üöÄ Starting training: {RUN_NAME}\")\n",
    "print(f\"Training for {TRAINING_CONFIG['epochs']} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG['epochs']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch + 1}/{TRAINING_CONFIG['epochs']}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        train_loader, \n",
    "        desc=f\"Epoch {epoch + 1}\",\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if TRAINING_CONFIG['use_amp']:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        if TRAINING_CONFIG['use_amp']:\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TRAINING_CONFIG['max_grad_norm'])\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TRAINING_CONFIG['max_grad_norm'])\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update tracking\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Log metrics\n",
    "        if global_step % TRAINING_CONFIG['logging_steps'] == 0:\n",
    "            training_stats['train_losses'].append(loss.item())\n",
    "            training_stats['learning_rates'].append(current_lr)\n",
    "            training_stats['steps'].append(global_step)\n",
    "        \n",
    "        # Evaluation\n",
    "        if global_step % TRAINING_CONFIG['eval_steps'] == 0:\n",
    "            print(f\"\\nüîç Evaluating at step {global_step}...\")\n",
    "            \n",
    "            val_metrics = evaluate_model(\n",
    "                model, val_loader, device, TRAINING_CONFIG['task']\n",
    "            )\n",
    "            \n",
    "            # Log validation metrics\n",
    "            training_stats['val_losses'].append(val_metrics['loss'])\n",
    "            training_stats['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            print(f\"Validation metrics:\")\n",
    "            for metric, value in val_metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "            \n",
    "            # Check for best model\n",
    "            current_f1 = val_metrics['f1']\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                epochs_without_improvement = 0\n",
    "                \n",
    "                # Save best model\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, global_step,\n",
    "                    val_metrics, best_model_path\n",
    "                )\n",
    "                print(f\"üéâ New best F1: {best_f1:.4f}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            model.train()  # Switch back to training mode\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if global_step % TRAINING_CONFIG['save_steps'] == 0:\n",
    "            checkpoint_path = f\"../results/checkpoints/checkpoint_{RUN_NAME}_step_{global_step}.pt\"\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, global_step,\n",
    "                val_metrics if 'val_metrics' in locals() else {},\n",
    "                checkpoint_path\n",
    "            )\n",
    "    \n",
    "    # End of epoch summary\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epochs_without_improvement >= TRAINING_CONFIG['early_stopping_patience']:\n",
    "        print(f\"\\n‚è∞ Early stopping triggered after {epochs_without_improvement} epochs without improvement\")\n",
    "        break\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüèÅ Training completed!\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Best F1 score: {best_f1:.4f}\")\n",
    "print(f\"Best model saved: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a71ee",
   "metadata": {},
   "source": [
    "## Final Model Evaluation and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50250282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "print(\"üîÑ Loading best model for final evaluation...\")\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Final validation evaluation\n",
    "print(\"\\nüìä Final validation evaluation:\")\n",
    "final_val_metrics = evaluate_model(model, val_loader, device, TRAINING_CONFIG['task'])\n",
    "\n",
    "for metric, value in final_val_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save final model (state dict only, for inference)\n",
    "final_model_path = f\"../models/codebert_final_{TRAINING_CONFIG['task']}_{TIMESTAMP}.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'task': TRAINING_CONFIG['task']\n",
    "    },\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'final_metrics': final_val_metrics,\n",
    "    'tokenizer_name': MODEL_NAME\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"\\nüíæ Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save training history\n",
    "training_history_path = f\"../results/metrics/training_history_{RUN_NAME}.json\"\n",
    "with open(training_history_path, 'w') as f:\n",
    "    # Convert numpy types to Python native types for JSON serialization\n",
    "    history_json = {}\n",
    "    for key, value in training_stats.items():\n",
    "        if isinstance(value, list) and len(value) > 0:\n",
    "            if isinstance(value[0], dict):\n",
    "                history_json[key] = [\n",
    "                    {k: float(v) if isinstance(v, (np.floating, np.integer)) else v \n",
    "                     for k, v in item.items()} \n",
    "                    for item in value\n",
    "                ]\n",
    "            else:\n",
    "                history_json[key] = [\n",
    "                    float(x) if isinstance(x, (np.floating, np.integer)) else x \n",
    "                    for x in value\n",
    "                ]\n",
    "        else:\n",
    "            history_json[key] = value\n",
    "    \n",
    "    json.dump(history_json, f, indent=2)\n",
    "\n",
    "print(f\"üìà Training history saved: {training_history_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3776602",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training visualization\n",
    "if len(training_stats['train_losses']) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training loss\n",
    "    if training_stats['train_losses']:\n",
    "        axes[0, 0].plot(training_stats['steps'], training_stats['train_losses'])\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Validation loss\n",
    "    if training_stats['val_losses']:\n",
    "        eval_steps = list(range(TRAINING_CONFIG['eval_steps'], \n",
    "                               len(training_stats['val_losses']) * TRAINING_CONFIG['eval_steps'] + 1, \n",
    "                               TRAINING_CONFIG['eval_steps']))\n",
    "        axes[0, 1].plot(eval_steps, training_stats['val_losses'])\n",
    "        axes[0, 1].set_title('Validation Loss')\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    if training_stats['learning_rates']:\n",
    "        axes[1, 0].plot(training_stats['steps'], training_stats['learning_rates'])\n",
    "        axes[1, 0].set_title('Learning Rate')\n",
    "        axes[1, 0].set_xlabel('Steps')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Validation F1 score\n",
    "    if training_stats['val_metrics']:\n",
    "        f1_scores = [m['f1'] for m in training_stats['val_metrics']]\n",
    "        eval_steps = list(range(TRAINING_CONFIG['eval_steps'], \n",
    "                               len(f1_scores) * TRAINING_CONFIG['eval_steps'] + 1, \n",
    "                               TRAINING_CONFIG['eval_steps']))\n",
    "        axes[1, 1].plot(eval_steps, f1_scores)\n",
    "        axes[1, 1].set_title('Validation F1 Score')\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('F1 Score')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        # Mark best F1\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        axes[1, 1].scatter(eval_steps[best_idx], f1_scores[best_idx], \n",
    "                          color='red', s=100, zorder=5, label=f'Best F1: {best_f1:.4f}')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = f\"../results/visualizations/training_curves_{RUN_NAME}.png\"\n",
    "    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Training curves saved: {plot_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738462e8",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'run_info': {\n",
    "        'run_name': RUN_NAME,\n",
    "        'timestamp': TIMESTAMP,\n",
    "        'task': TRAINING_CONFIG['task'],\n",
    "        'model_name': MODEL_NAME,\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'data_info': {\n",
    "        'train_samples': len(train_loader.dataset),\n",
    "        'val_samples': len(val_loader.dataset),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES\n",
    "    },\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'model_info': {\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_f1': best_f1,\n",
    "        'final_metrics': final_val_metrics,\n",
    "        'training_time_minutes': total_time / 60,\n",
    "        'total_steps': global_step\n",
    "    },\n",
    "    'file_paths': {\n",
    "        'best_model': best_model_path,\n",
    "        'final_model': final_model_path,\n",
    "        'training_history': training_history_path\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"../results/metrics/training_summary_{RUN_NAME}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"  Run: {RUN_NAME}\")\n",
    "print(f\"  Task: {TRAINING_CONFIG['task']} classification\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Training time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"  Total steps: {global_step:,}\")\n",
    "print(f\"  Best F1 score: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Files:\")\n",
    "print(f\"  Best model: {best_model_path}\")\n",
    "print(f\"  Final model: {final_model_path}\")\n",
    "print(f\"  Training history: {training_history_path}\")\n",
    "print(f\"  Summary: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"  1. Run comprehensive evaluation: 05_evaluation_and_metrics.ipynb\")\n",
    "print(f\"  2. Create attention visualizations: 06_attention_visualization.ipynb\")\n",
    "print(f\"  3. Benchmark against Slither/Mythril: 07_benchmark_vs_slither_mythril.ipynb\")\n",
    "\n",
    "print(f\"\\nüí° Usage Example for Loading Trained Model:\")\n",
    "print(f\"```python\")\n",
    "print(f\"import torch\")\n",
    "print(f\"from transformers import AutoTokenizer\")\n",
    "print(f\"\")\n",
    "print(f\"# Load tokenizer\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\")\n",
    "print(f\"\")\n",
    "print(f\"# Load model\")\n",
    "print(f\"checkpoint = torch.load('{final_model_path}')\")\n",
    "print(f\"model = CodeBERTClassifier(\")\n",
    "print(f\"    model_name='{MODEL_NAME}',\")\n",
    "print(f\"    num_classes={NUM_CLASSES}\")\n",
    "print(f\")\")\n",
    "print(f\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(f\"model.eval()\")\n",
    "print(f\"```\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
